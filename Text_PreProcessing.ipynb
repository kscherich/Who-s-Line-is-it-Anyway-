{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import en_core_web_sm\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the nlp pipeline from spaCy, and import the quotes dataframe (two column dataframe: Quote and Author)\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\MainUser\\Desktop\\Quotes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify quotes to strip away unnessary white space and make all words lowercase\n",
    "df['Quote'] = df['Quote'].apply(lambda qt : qt.strip())\n",
    "df['Quote'] = df['Quote'].apply(lambda qt : qt.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define two functions required to break long quotes into individual sentences\n",
    "def break_sentences(quote, author, df, index):\n",
    "    '''\n",
    "    Turns a single quote into a dataframe, with each sentence of the quote as a new line in the dataframe.\n",
    "    This dataframe is concatonated to an existing dataframe.  Sentences identified using the nlp library spaCy.\n",
    "\n",
    "    Args:\n",
    "        Quote (str): a quote to be broken into individual sentences. May be of any length.\n",
    "        Author (str): Author to be listed for the quote in the dataframe\n",
    "        df (dataframe): a dataframe with a column for quote and a column for author. May be blank or contain\n",
    "                        prior quotes already broken into sentences\n",
    "        index (int): starting index to add additional quotes to the dataframe\n",
    "\n",
    "    Returns:\n",
    "        df (dataframe): the input dataframe with a new line concatonated for each sentence in the input quote\n",
    "        index (int): starting index for any future quotes to be added to the dataframe\n",
    "    '''\n",
    "    doc = nlp(quote)\n",
    "    for sentence in list(doc.sents):\n",
    "        df_sentence = pd.DataFrame({'Quote':str(sentence),'Author':author},index=[index])\n",
    "        df = pd.concat([df,df_sentence])\n",
    "        index += 1\n",
    "    return df, index\n",
    "\n",
    "def break_long_quotes_into_sentences(df):\n",
    "    '''\n",
    "    Breaks each quote in the input dataframe into seperate quotes, each one sentence long, and\n",
    "    returns each of these new one-sentence quotes as a seperate line in the dataframe. Sentences\n",
    "    identified using the nlp library spaCy\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): a dataframe with a column for quote and a column for author. Quotes are of any length\n",
    "\n",
    "    Returns:\n",
    "        df_updated (dataframe): a dataframe with a column for quote and a column for author. Quotes are one sentence long.\n",
    "\n",
    "    '''\n",
    "    df_updated = pd.DataFrame(columns = ['Quote','Author'])\n",
    "    index = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        quote = df['Quote'][i]\n",
    "        author = df['Author'][i]\n",
    "        df_updated, index = break_sentences(quote, author, df_updated, index)\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break long quotes into individual sentences\n",
    "df = break_long_quotes_into_sentences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to execute the nlp pipeline to extract relevant information about a single quote\n",
    "def nlp_dict(quote):\n",
    "    '''\n",
    "    Uses the nlp library spaCy to:\n",
    "     -- Determine the language of the quote\n",
    "     -- Break the quote up into words\n",
    "     -- Identify if each word is a stop word\n",
    "     -- Identify the lemma for each word\n",
    "    In addition, this calculates the length of the quote, and the fraction of words in the quote that are stop words\n",
    "    \n",
    "    Args:\n",
    "        quote (str): a sentence-length quote \n",
    "    \n",
    "    Returns:\n",
    "        nlp_dict (dict): A dictionary containing the following for the quote:\n",
    "        language (str), language_score (float), sentence_length (int),\n",
    "        stop_word_pct (float), words (list), words_ex_stopwords (list), and lemmas (list)\n",
    "    \n",
    "    '''\n",
    "    doc = nlp(quote)\n",
    "    language = doc._.language['language']\n",
    "    language_score = doc._.language['score']\n",
    "    words = []\n",
    "    words_ex_stopwords = []\n",
    "    lemmas = []\n",
    "    sentence_length = 0\n",
    "    stop_word_count = 0\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            sentence_length += 1\n",
    "            words.append(token.text)\n",
    "            if not token.is_stop: \n",
    "                words_ex_stopwords.append(token.text)\n",
    "                lemmas.append(token.lemma_)\n",
    "            if token.is_stop:\n",
    "                stop_word_count += 1\n",
    "    if sentence_length > 0:\n",
    "        stop_word_pct = stop_word_count/ sentence_length\n",
    "    else:\n",
    "        stop_word_pct = np.nan\n",
    "    return dict(language = language,\n",
    "                language_score = language_score,\n",
    "                sentence_length = sentence_length,\n",
    "                stop_word_pct = stop_word_pct,\n",
    "                words = words,\n",
    "                words_ex_stopwords = words_ex_stopwords,\n",
    "                lemmas = lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting relevant information about each quote using the function above and saving each item of the dictionary to\n",
    "#its own column in the dataframe\n",
    "df['dict'] = df['Quote'].apply(nlp_dict)\n",
    "df['language'] = df['dict'].apply(lambda x: x['language'])\n",
    "df['language_score'] = df['dict'].apply(lambda x: x['language_score'])\n",
    "df['sentence_length'] = df['dict'].apply(lambda x: x['sentence_length'])\n",
    "df['stop_word_pct'] = df['dict'].apply(lambda x: x['stop_word_pct'])\n",
    "df['words'] = df['dict'].apply(lambda x: x['words'])\n",
    "df['words_ex_stopwords'] = df['dict'].apply(lambda x: x['words_ex_stopwords'])\n",
    "df['lemmas'] = df['dict'].apply(lambda x: x['lemmas'])\n",
    "df['lemmas_text'] = df['lemmas'].apply(lambda x: ' '.join(x))\n",
    "df.drop(columns = ['dict'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping quotes where there isn't at least one non-stop word\n",
    "df.drop(df[df['lemmas'].apply(len)== 0].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicates Dropped: 1720\n",
      "Fraction Duplicates: 0.13756698392385824\n"
     ]
    }
   ],
   "source": [
    "#dropping duplicate quotes, and printing information on the number of duplicate quotes dropped\n",
    "index_to_keep = df.astype(str).drop_duplicates(['words'],keep='first').index\n",
    "print(\"Number of Duplicates Dropped: \"+ str(len(df.index) - len(index_to_keep)))\n",
    "print(\"Fraction Duplicates: \"+ str(1-len(index_to_keep)/len(df.index)))\n",
    "df.drop(set(df.index) - set(index_to_keep),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Non-English Quotes\n",
    "\n",
    "### After examining Spacy's language classification of quotes, it became clear we needed a better system to clean the quotes than just deleting all non-English quotes.\n",
    "\n",
    "I found a better method for language detection was to use a combination of Spacy's language classification and my stop words percentage metric to determine non-English quotes *(stop words are all English, so non-English quotes are likely to have a small percentage of stop words)*. Removing non-English quotes is done on a language-by-language basis because of the differences across languages. This process can be followed in the steps below (starting with the least common languages and building up to the most common)\n",
    "\n",
    "Note that many of these cells have not been executed in the version uploaded to github so viewing the code is managable, but execution of the code would return a dataframe which is then visually examined to determine which quotes to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>language</th>\n",
       "      <th>language_score</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>stop_word_pct</th>\n",
       "      <th>words</th>\n",
       "      <th>words_ex_stopwords</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>“in vain have i struggled.</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>da</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[in, vain, have, i, struggled]</td>\n",
       "      <td>[vain, struggled]</td>\n",
       "      <td>[vain, struggle]</td>\n",
       "      <td>vain struggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i require so much!”</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>ca</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[i, require, so, much]</td>\n",
       "      <td>[require]</td>\n",
       "      <td>[require]</td>\n",
       "      <td>require</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>it is too long ago.</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>tl</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[it, is, too, long, ago]</td>\n",
       "      <td>[long, ago]</td>\n",
       "      <td>[long, ago]</td>\n",
       "      <td>long ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>“you pierce my soul.</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[you, pierce, my, soul]</td>\n",
       "      <td>[pierce, soul]</td>\n",
       "      <td>[pierce, soul]</td>\n",
       "      <td>pierce soul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>i am half agony, half hope...</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>cy</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[i, am, half, agony, half, hope]</td>\n",
       "      <td>[half, agony, half, hope]</td>\n",
       "      <td>[half, agony, half, hope]</td>\n",
       "      <td>half agony half hope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Quote       Author language  language_score  \\\n",
       "8      “in vain have i struggled.  Jane Austen       da        0.999993   \n",
       "14            i require so much!”  Jane Austen       ca        0.571429   \n",
       "19            it is too long ago.  Jane Austen       tl        0.999996   \n",
       "28           “you pierce my soul.  Jane Austen       fr        0.999997   \n",
       "29  i am half agony, half hope...  Jane Austen       cy        0.999996   \n",
       "\n",
       "    sentence_length  stop_word_pct                             words  \\\n",
       "8                 5       0.600000    [in, vain, have, i, struggled]   \n",
       "14                4       0.750000            [i, require, so, much]   \n",
       "19                5       0.600000          [it, is, too, long, ago]   \n",
       "28                4       0.500000           [you, pierce, my, soul]   \n",
       "29                6       0.333333  [i, am, half, agony, half, hope]   \n",
       "\n",
       "           words_ex_stopwords                     lemmas           lemmas_text  \n",
       "8           [vain, struggled]           [vain, struggle]         vain struggle  \n",
       "14                  [require]                  [require]               require  \n",
       "19                [long, ago]                [long, ago]              long ago  \n",
       "28             [pierce, soul]             [pierce, soul]           pierce soul  \n",
       "29  [half, agony, half, hope]  [half, agony, half, hope]  half agony half hope  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Many of the quotes SpaCy classified as non-English quotes appear to actually be English!\n",
    "df[df['language'] != 'en'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    8767\n",
       "es     609\n",
       "it     240\n",
       "fr     144\n",
       "cy     137\n",
       "pt     112\n",
       "af      89\n",
       "no      67\n",
       "de      62\n",
       "da      58\n",
       "nl      47\n",
       "id      45\n",
       "ro      42\n",
       "tr      42\n",
       "ca      39\n",
       "et      38\n",
       "tl      37\n",
       "hr      34\n",
       "so      31\n",
       "fi      20\n",
       "pl      15\n",
       "vi      14\n",
       "sq      11\n",
       "el      11\n",
       "lt      11\n",
       "cs      10\n",
       "sl      10\n",
       "sw       9\n",
       "hu       8\n",
       "sv       7\n",
       "sk       7\n",
       "ru       5\n",
       "lv       2\n",
       "fa       1\n",
       "bg       1\n",
       "ja       1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start by reviewing the least common languages all together, and then review the top 14 on a language-by-language basis\n",
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the pandas display setting to show all of the rows, and then show only the least common languages. My assessment:\n",
    "# drop: the languages other the the top 14 with stop word percent < 22%, plus rows 4685, 4667, 9115, 5433, 2268, 3976, 1245. \n",
    "# Drops ~20 english quotes.\n",
    "pd.set_option('display.max_rows', None)\n",
    "cond1 = df['language'] != 'en'\n",
    "cond2 = df['language'] != 'es'\n",
    "cond3 = df['language'] != 'it'\n",
    "cond4 = df['language'] != 'fr'\n",
    "cond5 = df['language'] != 'cy'\n",
    "cond6 = df['language'] != 'pt'\n",
    "cond7 = df['language'] != 'af'\n",
    "cond8 = df['language'] != 'no'\n",
    "cond9 = df['language'] != 'de'\n",
    "cond10 = df['language'] != 'da'\n",
    "cond11 = df['language'] != 'nl'\n",
    "cond12 = df['language'] != 'id'\n",
    "cond13 = df['language'] != 'ro'\n",
    "cond14 = df['language'] != 'tr'\n",
    "df[(cond1)&(cond2)&(cond3)&(cond4)&(cond5)&(cond6)&(cond7)&(cond8)&(cond9)&(cond10)&(cond11)&(cond12)&(cond13)&(cond14)].sort_values(by=['stop_word_pct','language'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'tr'].sort_values(by=['stop_word_pct'])\n",
    "## drop 'tr' with stop word percent < 30%. Drops ~ 5 english quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'ro'].sort_values(by=['stop_word_pct'])\n",
    "## drop 'ro' with stop word percent < 30%, and row 11499. Drops ~5 english quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'id'].sort_values(by=['stop_word_pct'])\n",
    "## drop 'id' with stop word percent < 30%. Drops ~2 english quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'nl'].sort_values(by=['stop_word_pct'])\n",
    "## keep 'nl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'da'].sort_values(by=['stop_word_pct'])\n",
    "## keep 'da'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'de'].sort_values(by=['stop_word_pct'])\n",
    "## drop 'de' with stop word percent < 30%. Drops ~5 english quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'no'].sort_values(by=['stop_word_pct'])\n",
    "## keep 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'af'].sort_values(by=['stop_word_pct'])\n",
    "## keep 'af'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'pt'].sort_values(by=['stop_word_pct'])\n",
    "## drop 'pt' with stop word percent < 30%, and rows 5813, 5827, 5801, 5028. Drops ~5 english quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'cy'].sort_values(by=['stop_word_pct'])\n",
    "## keep 'cy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'fr'].sort_values(by=['stop_word_pct'])\n",
    "# drop 'fr' with stop word percent < 25%. Drops ~10 english quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'it'].sort_values(by=['stop_word_pct'])\n",
    "# drop 'it' with stop word percent < 41%, and row 3464. I observed 5 english quotes dropped, but I also\n",
    "# didn't scroll as carefully through all of these quotes. I excpet there are < 10 english quotes dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'es'].sort_values(by=['stop_word_pct'])\n",
    "# drop 'es' with stop word percent < 39%, and rows 2872, 1251. I didn't observe any english quotes dropped, but I also\n",
    "# didn't scroll as carefully through all of these quotes. I excpet there are < 5 english quotes dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['language'] == 'en'].sort_values(by=['stop_word_pct'])\n",
    "## glancing through there doesn't seem to be any misclassified, but let's take a closer look at longer sentence with no\n",
    "## stop words, as they are the most likely to be misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = df['language'] == 'en'\n",
    "cond2 = df['stop_word_pct'] < 0.15\n",
    "cond3 = df['sentence_length'] > 5\n",
    "df[(cond1)&(cond2)&(cond3)]\n",
    "## should drop row 5660. It's english, but not really a quote per say, and row 7608 doesn't appear to be english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the non-English quotes identified in the steps above. Estimated total number of english quotes dropped is 60 quotes.\n",
    "\n",
    "# dropping top 14 languages with specified stop word percents\n",
    "to_drop_lang_pct = [('es',.39),('it',.41),('fr',.25),('pt',.3),('de',.3),('tr',.3),('ro',.3),('id',.3)]\n",
    "for tup in to_drop_lang_pct:\n",
    "    lang = tup[0]\n",
    "    pct = tup[1]\n",
    "    lang_condition = df['language'] == lang\n",
    "    stop_words_condition = df['stop_word_pct'] < pct\n",
    "    df.drop(df[(lang_condition)&(stop_words_condition)].index, inplace=True)\n",
    "\n",
    "# dropping spotted outlier rows    \n",
    "to_drop_index = [4685,4667,9115,5433,2268,3976,1245,11499,5813,5827,5801,5028,3464,5660,7680]\n",
    "df.drop(to_drop_index,inplace=True)\n",
    "\n",
    "# dropping less-common languages (below the top 14) with stop word percentage less than 22%\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "to_drop_top14langs = ['en','es','it','fr','cy','pt','af','no','de','da','nl','tr','ro','id']\n",
    "lang_cond_inv = []\n",
    "for i in range(len(to_drop_top14langs)):\n",
    "    lang = to_drop_top14langs[i]\n",
    "    lang_cond_inv.append(df['language'] != lang)\n",
    "lang_cond = (lang_cond_inv[0])\n",
    "for i in range(1, len(lang_cond_inv)):\n",
    "    lang_cond = (lang_cond) & (lang_cond_inv[i])\n",
    "stop_words_cond = df['stop_word_pct'] < .22    \n",
    "df.drop(df[(lang_cond)&(stop_words_cond)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving clened quotes dataframe to CSV\n",
    "df.to_csv(path_or_buf=r'C:\\Users\\MainUser\\Desktop\\Quotes_Cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
