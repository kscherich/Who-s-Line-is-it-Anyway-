{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to scrope quotes from a single webpage\n",
    "def get_page_quotes(url):\n",
    "    '''\n",
    "    Scapes all of the quotes from a single page of goodreads quotes\n",
    "    \n",
    "    Args:\n",
    "        url (text): a url to a single page of quotes on goodreads\n",
    "        format \"https://www.goodreads.com/author/quotes/[AUTHOR SPECIFIC INFO]?page=[NUMBER]\"\n",
    "    \n",
    "    Returns:\n",
    "        quotes (list): a list of all the quotes on the given url page\n",
    "    '''\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.text, \"html.parser\")\n",
    "    quotes = []\n",
    "\n",
    "    for div in soup.findAll('div', class_='quoteText'):\n",
    "        quotes.append(div.find_all(text=True)[0])\n",
    "\n",
    "    return quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to scrope pages of quotes for a single author\n",
    "def scrape_all_pages(url_base, author, numPages):\n",
    "    '''\n",
    "    Scapes all of the quotes for a particular author (specified in url_base) from the goodreads quotes page starting\n",
    "    from the first page of quotes to the indicated last page of quotes (last page specified as numPages). Returns a two\n",
    "    column dataframe containing quotes in one column, and the author name (specified as author) in the other column\n",
    "    \n",
    "    Args:\n",
    "        url_base (text): goodreads quotes url for a specific author, with page info removed\n",
    "                         format \"https://www.goodreads.com/author/quotes/[AUTHOR SPECIFIC INFO]?page=\"\n",
    "        author (str):   Name of author being scraped\n",
    "        numPages (int): number of pages of quotes to be scraped\n",
    "                        must be less than or equal to the total number of pages that exist for the author\n",
    "    \n",
    "    Returns:\n",
    "        quotes (dataframe): a two column dataframe containing: (1) a list of all the quotes from the first\n",
    "                            page of quotes to the indicated last page of quotes. Note that some pages may\n",
    "                            be skipped due to an internet timeout exception, print statements will indicate\n",
    "                            if pages have been successfully scraped or if they failed to scrape. (2) the\n",
    "                            author name as written in the args to the function\n",
    "    '''    \n",
    "    quotes = []\n",
    "    for page in range (1,numPages+1):\n",
    "        url = url_base + str(page)\n",
    "        try:\n",
    "            quotes.extend(get_page_quotes(url))\n",
    "            print('Successfully Scraped Page #'+str(page))\n",
    "        except:\n",
    "            ## sometimes my internet timesout for a small window, and I don't care that I scrape every page\n",
    "            ## so I just pass if it's taking too long to respond\n",
    "            print('Failed to Scrape Page #'+str(page))\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(data = quotes, columns = ['Quote'])\n",
    "    df['Author'] = author\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining input parameters for the scrape_all_pages function for both authors\n",
    "Jane_Austen_url_base = 'https://www.goodreads.com/author/quotes/1265.Jane_Austen?page='\n",
    "JK_Rowling_url_base = 'https://www.goodreads.com/author/quotes/1077326.J_K_Rowling?page='\n",
    "numPages = 100 ## Jane Austen has ~120 pages; JK Rowling has ~210 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Scraped Page #1\n",
      "Successfully Scraped Page #2\n",
      "Successfully Scraped Page #3\n",
      "Successfully Scraped Page #4\n",
      "Successfully Scraped Page #5\n",
      "Successfully Scraped Page #6\n",
      "Successfully Scraped Page #7\n",
      "Successfully Scraped Page #8\n",
      "Successfully Scraped Page #9\n",
      "Successfully Scraped Page #10\n",
      "Failed to Scrape Page #11\n",
      "Failed to Scrape Page #12\n",
      "Successfully Scraped Page #13\n",
      "Failed to Scrape Page #14\n",
      "Successfully Scraped Page #15\n",
      "Successfully Scraped Page #16\n",
      "Successfully Scraped Page #17\n",
      "Successfully Scraped Page #18\n",
      "Successfully Scraped Page #19\n",
      "Successfully Scraped Page #20\n",
      "Successfully Scraped Page #21\n",
      "Successfully Scraped Page #22\n",
      "Successfully Scraped Page #23\n",
      "Successfully Scraped Page #24\n",
      "Successfully Scraped Page #25\n",
      "Successfully Scraped Page #26\n",
      "Successfully Scraped Page #27\n",
      "Successfully Scraped Page #28\n",
      "Successfully Scraped Page #29\n",
      "Successfully Scraped Page #30\n",
      "Successfully Scraped Page #31\n",
      "Successfully Scraped Page #32\n",
      "Successfully Scraped Page #33\n",
      "Successfully Scraped Page #34\n",
      "Successfully Scraped Page #35\n",
      "Successfully Scraped Page #36\n",
      "Successfully Scraped Page #37\n",
      "Successfully Scraped Page #38\n",
      "Successfully Scraped Page #39\n",
      "Successfully Scraped Page #40\n",
      "Successfully Scraped Page #41\n",
      "Successfully Scraped Page #42\n",
      "Successfully Scraped Page #43\n",
      "Successfully Scraped Page #44\n",
      "Successfully Scraped Page #45\n",
      "Successfully Scraped Page #46\n",
      "Successfully Scraped Page #47\n",
      "Successfully Scraped Page #48\n",
      "Successfully Scraped Page #49\n",
      "Successfully Scraped Page #50\n",
      "Successfully Scraped Page #51\n",
      "Successfully Scraped Page #52\n",
      "Successfully Scraped Page #53\n",
      "Successfully Scraped Page #54\n",
      "Successfully Scraped Page #55\n",
      "Successfully Scraped Page #56\n",
      "Successfully Scraped Page #57\n",
      "Successfully Scraped Page #58\n",
      "Successfully Scraped Page #59\n",
      "Successfully Scraped Page #60\n",
      "Successfully Scraped Page #61\n",
      "Successfully Scraped Page #62\n",
      "Successfully Scraped Page #63\n",
      "Successfully Scraped Page #64\n",
      "Successfully Scraped Page #65\n",
      "Successfully Scraped Page #66\n",
      "Successfully Scraped Page #67\n",
      "Successfully Scraped Page #68\n",
      "Successfully Scraped Page #69\n",
      "Successfully Scraped Page #70\n",
      "Successfully Scraped Page #71\n",
      "Failed to Scrape Page #72\n",
      "Successfully Scraped Page #73\n",
      "Successfully Scraped Page #74\n",
      "Successfully Scraped Page #75\n",
      "Successfully Scraped Page #76\n",
      "Successfully Scraped Page #77\n",
      "Successfully Scraped Page #78\n",
      "Successfully Scraped Page #79\n",
      "Successfully Scraped Page #80\n",
      "Successfully Scraped Page #81\n",
      "Successfully Scraped Page #82\n",
      "Successfully Scraped Page #83\n",
      "Successfully Scraped Page #84\n",
      "Successfully Scraped Page #85\n",
      "Successfully Scraped Page #86\n",
      "Successfully Scraped Page #87\n",
      "Successfully Scraped Page #88\n",
      "Successfully Scraped Page #89\n",
      "Successfully Scraped Page #90\n",
      "Successfully Scraped Page #91\n",
      "Successfully Scraped Page #92\n",
      "Successfully Scraped Page #93\n",
      "Successfully Scraped Page #94\n",
      "Successfully Scraped Page #95\n",
      "Successfully Scraped Page #96\n",
      "Successfully Scraped Page #97\n",
      "Successfully Scraped Page #98\n",
      "Successfully Scraped Page #99\n",
      "Successfully Scraped Page #100\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scrape_all_page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-73f41909c7c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mJane_quotes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_all_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJane_Austen_url_base\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Jane Austen'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumPages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mJK_quotes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_all_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJK_Rowling_url_base\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'JK Rowling'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scrape_all_page' is not defined"
     ]
    }
   ],
   "source": [
    "# scraping all pages for Jane Austen. When I ran, failed to scrape pages 11, 12, 14, 72\n",
    "Jane_quotes = scrape_all_pages(Jane_Austen_url_base,'Jane Austen',numPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Scraped Page #1\n",
      "Successfully Scraped Page #2\n",
      "Successfully Scraped Page #3\n",
      "Successfully Scraped Page #4\n",
      "Successfully Scraped Page #5\n",
      "Successfully Scraped Page #6\n",
      "Successfully Scraped Page #7\n",
      "Successfully Scraped Page #8\n",
      "Successfully Scraped Page #9\n",
      "Successfully Scraped Page #10\n",
      "Successfully Scraped Page #11\n",
      "Successfully Scraped Page #12\n",
      "Successfully Scraped Page #13\n",
      "Successfully Scraped Page #14\n",
      "Successfully Scraped Page #15\n",
      "Successfully Scraped Page #16\n",
      "Successfully Scraped Page #17\n",
      "Successfully Scraped Page #18\n",
      "Successfully Scraped Page #19\n",
      "Successfully Scraped Page #20\n",
      "Successfully Scraped Page #21\n",
      "Successfully Scraped Page #22\n",
      "Failed to Scrape Page #23\n",
      "Successfully Scraped Page #24\n",
      "Successfully Scraped Page #25\n",
      "Successfully Scraped Page #26\n",
      "Successfully Scraped Page #27\n",
      "Successfully Scraped Page #28\n",
      "Successfully Scraped Page #29\n",
      "Successfully Scraped Page #30\n",
      "Successfully Scraped Page #31\n",
      "Successfully Scraped Page #32\n",
      "Successfully Scraped Page #33\n",
      "Successfully Scraped Page #34\n",
      "Successfully Scraped Page #35\n",
      "Successfully Scraped Page #36\n",
      "Successfully Scraped Page #37\n",
      "Successfully Scraped Page #38\n",
      "Successfully Scraped Page #39\n",
      "Successfully Scraped Page #40\n",
      "Successfully Scraped Page #41\n",
      "Successfully Scraped Page #42\n",
      "Successfully Scraped Page #43\n",
      "Successfully Scraped Page #44\n",
      "Successfully Scraped Page #45\n",
      "Successfully Scraped Page #46\n",
      "Successfully Scraped Page #47\n",
      "Successfully Scraped Page #48\n",
      "Successfully Scraped Page #49\n",
      "Successfully Scraped Page #50\n",
      "Successfully Scraped Page #51\n",
      "Successfully Scraped Page #52\n",
      "Successfully Scraped Page #53\n",
      "Successfully Scraped Page #54\n",
      "Successfully Scraped Page #55\n",
      "Successfully Scraped Page #56\n",
      "Successfully Scraped Page #57\n",
      "Successfully Scraped Page #58\n",
      "Successfully Scraped Page #59\n",
      "Successfully Scraped Page #60\n",
      "Successfully Scraped Page #61\n",
      "Successfully Scraped Page #62\n",
      "Successfully Scraped Page #63\n",
      "Successfully Scraped Page #64\n",
      "Successfully Scraped Page #65\n",
      "Successfully Scraped Page #66\n",
      "Successfully Scraped Page #67\n",
      "Successfully Scraped Page #68\n",
      "Successfully Scraped Page #69\n",
      "Successfully Scraped Page #70\n",
      "Successfully Scraped Page #71\n",
      "Successfully Scraped Page #72\n",
      "Successfully Scraped Page #73\n",
      "Successfully Scraped Page #74\n",
      "Successfully Scraped Page #75\n",
      "Successfully Scraped Page #76\n",
      "Successfully Scraped Page #77\n",
      "Failed to Scrape Page #78\n",
      "Successfully Scraped Page #79\n",
      "Successfully Scraped Page #80\n",
      "Successfully Scraped Page #81\n",
      "Successfully Scraped Page #82\n",
      "Successfully Scraped Page #83\n",
      "Successfully Scraped Page #84\n",
      "Successfully Scraped Page #85\n",
      "Successfully Scraped Page #86\n",
      "Successfully Scraped Page #87\n",
      "Failed to Scrape Page #88\n",
      "Successfully Scraped Page #89\n",
      "Failed to Scrape Page #90\n",
      "Successfully Scraped Page #91\n",
      "Successfully Scraped Page #92\n",
      "Successfully Scraped Page #93\n",
      "Successfully Scraped Page #94\n",
      "Successfully Scraped Page #95\n",
      "Successfully Scraped Page #96\n",
      "Successfully Scraped Page #97\n",
      "Successfully Scraped Page #98\n",
      "Successfully Scraped Page #99\n",
      "Successfully Scraped Page #100\n"
     ]
    }
   ],
   "source": [
    "# scraping all pages for JK Rowling. When I ran, failed to scrape pages 23, 78, 88, 90\n",
    "JK_quotes = scrape_all_pages(JK_Rowling_url_base,'JK Rowling',numPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the qutoes to a CSV file\n",
    "df = pd.concat([Jane_quotes,JK_quotes])\n",
    "df.to_csv(path_or_buf=r'C:\\Users\\MainUser\\Desktop\\Quotes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
